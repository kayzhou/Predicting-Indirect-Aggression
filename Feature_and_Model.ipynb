{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用户筛选和基本信息提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import pendulum\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽取用户时间特征\n",
    "\n",
    "def day_night(time_list):\n",
    "    '''\n",
    "    发微博的时间序列转化为24个时间段的统计\n",
    "    :param in_name:\n",
    "    :param out_name:\n",
    "    :return: \n",
    "    '''\n",
    "    cnt = [0] * 24\n",
    "    for t in time_list:\n",
    "        hour = t.hour\n",
    "        # print(hour)\n",
    "        # 分为四个时间段\n",
    "        # cnt[int(int(hour) / 6)] += 1\n",
    "        # 分为24小时\n",
    "        cnt[int(hour)] += 1\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def weeks(time_list):\n",
    "    '''\n",
    "    发微博的时间序列转化为7个时间段的统计\n",
    "    :param in_name:\n",
    "    :param out_name:\n",
    "    :return:\n",
    "    '''\n",
    "    cnt = [0] * 7\n",
    "    for t in time_list:\n",
    "        weekday = t.weekday()\n",
    "        cnt[int(weekday)] += 1\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def time_interval(time_list):\n",
    "    interval = []\n",
    "    for i in np.arange(1, len(time_list)):\n",
    "        t2 = time_list[i-1]\n",
    "        t1 = time_list[i]\n",
    "        _inter = abs((t2 - t1).total_seconds())\n",
    "        if _inter < 86400 * 7:\n",
    "            interval.append(_inter / 3600)\n",
    "    # return interval\n",
    "    interval = np.array(interval)\n",
    "    # print(interval)\n",
    "    return interval\n",
    "\n",
    "\n",
    "def life_length(time_list):\n",
    "    return int((max(time_list) - min(time_list)).total_seconds() / 3600 / 24) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"data/weibo_data_456_json\"\n",
    "\n",
    "users_n = []\n",
    "\n",
    "# 筛选出发微博超过20条的，且最近一条微博为2020年以后发布，且第一条与最后一条微博间隔天数超过60天的用户\n",
    "for i, in_name in enumerate(tqdm(Path(in_dir).rglob(\"*.json\"))):\n",
    "    d = json.load(open(in_name,encoding='utf-8'))\n",
    "    user = d[\"user\"]\n",
    "    weibos = d[\"weibo\"]\n",
    "    time_list = [pendulum.parse(w[\"publish_time\"]) for w in weibos]\n",
    "    recent = time_list[0]\n",
    "    span = (time_list[0]-time_list[len(time_list)-1]).days\n",
    "    if len(weibos)>=20 and recent.year>=2020 and span>=60:\n",
    "        users_n.append(user[\"id\"])\n",
    "\n",
    "len(users_n)  #一共320个用户满足条件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取用户的微博其他特征：基本信息，所有微博，转发微博，提及微博，发图的数量和比例\n",
    "\n",
    "in_dir = \"data/weibo_data_456_json\"\n",
    "\n",
    "users_features = []\n",
    "\n",
    "for i, in_name in enumerate(tqdm(Path(in_dir).rglob(\"*.json\"))):\n",
    "    if not in_name.name[:-5] in users_n:\n",
    "        continue\n",
    "    d = json.load(open(in_name,encoding='utf-8'))\n",
    "    user = d[\"user\"]\n",
    "    u = {\n",
    "        \"uid\": user[\"id\"],\n",
    "        \"gender\": 1 if user[\"gender\"]==\"男\" else 0,\n",
    "        \"have_talent\": int(user[\"talent\"] != \"\"),\n",
    "        \"have_education\": int(user[\"education\"] != \"\"),\n",
    "        \"have_work\": int(user[\"work\"] != \"\"),\n",
    "        \"NT\": np.log(user[\"weibo_num\"] + 1),\n",
    "        \"NFEE\": np.log(user[\"following\"] + 1),\n",
    "        \"NEER\": np.log(user[\"followers\"] + 1),\n",
    "        \"NT/NFEE\": np.log((user[\"weibo_num\"] + 1) / (user[\"following\"] + 1)),\n",
    "        \"NT/NFER\": np.log((user[\"weibo_num\"] + 1) / (user[\"followers\"] + 1)),\n",
    "        \"NEEE/NFER\": np.log((user[\"following\"] + 1) / (user[\"followers\"] + 1)),\n",
    "        \"len_desc\": len(user[\"description\"]),\n",
    "    }\n",
    "    weibos = d[\"weibo\"]\n",
    "\n",
    "    \n",
    "    # 所有微博\n",
    "    #d_features 日时间特征\n",
    "    time_list = [pendulum.parse(w[\"publish_time\"]) for w in weibos]\n",
    "    d_features = np.array(day_night(time_list))\n",
    "    for i, _d in enumerate(d_features):\n",
    "        u[f\"h_{i}\"] = _d\n",
    "    u[\"h_max\"] = d_features.max()\n",
    "    u[\"h_argmax\"] = d_features.argmax()\n",
    "    u[\"h_std\"] = d_features.std()\n",
    "    #w_features 周时间特征\n",
    "    w_features = np.array(weeks(time_list))\n",
    "    for i, _d in enumerate(w_features):\n",
    "        u[f\"w_{i}\"] = _d\n",
    "    u[\"w_max\"] = w_features.max()\n",
    "    u[\"w_argmax\"] = w_features.argmax()\n",
    "    u[\"w_std\"] = w_features.std()\n",
    "    #interval 发博间隔时间特征\n",
    "    interval = time_interval(time_list)\n",
    "    if len(interval) < 1:\n",
    "        u[\"interval_mean\"] = 0\n",
    "        u[\"interval_std\"] = 0\n",
    "    else:\n",
    "        u[\"interval_mean\"] = interval.mean()\n",
    "        u[\"interval_std\"] = interval.std()\n",
    "\n",
    "    u[\"life_length\"] = np.log(life_length(time_list) + 1)\n",
    "    u[\"ave_d_num\"] = user[\"weibo_num\"] / u[\"life_length\"]\n",
    "\n",
    "    \n",
    "    # 转发微博\n",
    "    time_list = [pendulum.parse(w[\"publish_time\"]) for w in weibos if not w[\"original\"]]\n",
    "    u[\"ret_prop\"] = len(time_list) / (user[\"weibo_num\"] + 1)\n",
    "    d_features = np.array(day_night(time_list))\n",
    "    for i, _d in enumerate(d_features):\n",
    "        u[f\"ret_h_{i}\"] = _d\n",
    "    u[\"ret_h_max\"] = d_features.max()\n",
    "    u[\"ret_h_argmax\"] = d_features.argmax()\n",
    "    u[\"ret_h_std\"] = d_features.std()\n",
    "     \n",
    "    w_features = np.array(weeks(time_list))\n",
    "    for i, _d in enumerate(w_features):\n",
    "        u[f\"ret_w_{i}\"] = _d\n",
    "    u[\"ret_w_max\"] = w_features.max()\n",
    "    u[\"ret_w_argmax\"] = w_features.argmax()\n",
    "    u[\"ret_w_std\"] = w_features.std()\n",
    "\n",
    "    interval = time_interval(time_list)\n",
    "    if len(interval) < 1:\n",
    "        u[\"ret_interval_mean\"] = 0\n",
    "        u[\"ret_interval_std\"] = 0\n",
    "    else:\n",
    "        u[\"ret_interval_mean\"] = interval.mean()\n",
    "        u[\"ret_interval_std\"] = interval.std()\n",
    "        \n",
    "    # 提及（@）微博\n",
    "    time_list = [pendulum.parse(w[\"publish_time\"]) for w in weibos if \"@\" in w[\"content\"]]\n",
    "    u[\"men_prop\"] = len(time_list) / (user[\"weibo_num\"] + 1)\n",
    "    d_features = np.array(day_night(time_list))\n",
    "    for i, _d in enumerate(d_features):\n",
    "        u[f\"men_h_{i}\"] = _d\n",
    "    u[\"men_h_max\"] = d_features.max()\n",
    "    u[\"men_h_argmax\"] = d_features.argmax()\n",
    "    u[\"men_h_std\"] = d_features.std()\n",
    "    \n",
    "    w_features = np.array(weeks(time_list))\n",
    "    for i, _d in enumerate(w_features):\n",
    "        u[f\"men_w_{i}\"] = _d\n",
    "    u[\"men_w_max\"] = w_features.max()\n",
    "    u[\"men_w_argmax\"] = w_features.argmax()\n",
    "    u[\"men_w_std\"] = w_features.std()\n",
    "\n",
    "    interval = time_interval(time_list)\n",
    "    if len(interval) < 1:\n",
    "        u[\"men_interval_mean\"] = 0\n",
    "        u[\"men_interval_std\"] = 0\n",
    "    else:\n",
    "        u[\"men_interval_mean\"] = interval.mean()\n",
    "        u[\"men_interval_std\"] = interval.std()\n",
    "        \n",
    "    # 发图数量和比例\n",
    "    pic_num = 0\n",
    "    pic_y = 0\n",
    "    for w in weibos:\n",
    "        if w[\"original_pictures\"] != \"无\": \n",
    "            pic_y += 1\n",
    "            pic_num += len(w[\"original_pictures\"].split(\",\"))\n",
    "    pic_prop = pic_y/len(weibos)\n",
    "    u[\"pic_num\"] = np.log(pic_num+1)\n",
    "    u[\"pic_prop\"] = pic_prop    \n",
    "    \n",
    "    \n",
    "    # 文本特征\n",
    "    # 另外的文件\n",
    "    users_features.append(u)\n",
    "    # print(user)\n",
    "    \n",
    "# len(users_features)\n",
    "\n",
    "df = pd.DataFrame(users_features).set_index(\"uid\")\n",
    "#df.to_csv(\"features_202207/users320_b+t_features_202210.csv\", float_format=\"%.4f\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import jieba\n",
    "from pandas.core.frame import DataFrame\n",
    "from tqdm.notebook import tqdm\n",
    "import pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把词库读成字典形式\n",
    "wl = []   #储存词\n",
    "vecl = []   #储存词向量\n",
    "weibo_word = open(\"sgns.weibo.word/sgns.weibo.word\",encoding='utf-8')\n",
    "next(weibo_word)     #第一行数据信息不要\n",
    "line = weibo_word.readline()   #一行一行读入\n",
    "while line:\n",
    "    list_w = line.split()\n",
    "    wl.append(list_w[0])\n",
    "    vec = []\n",
    "    for i in range(1,301):\n",
    "        vec.append(float(list_w[i]))\n",
    "    vecl.append(vec)\n",
    "    line = weibo_word.readline()\n",
    "weibo_word.close()\n",
    "\n",
    "\n",
    "word_dict = dict(zip(wl,vecl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"data/weibo_data_456_json\"\n",
    "\n",
    "users_n = []\n",
    "\n",
    "# 筛选出发微博超过20条的，且最近一条微博为2020年以后发布，且第一条与最后一条微博间隔天数超过60天的用户\n",
    "for i, in_name in enumerate(tqdm(Path(in_dir).rglob(\"*.json\"))):\n",
    "    d = json.load(open(in_name,encoding='utf-8'))\n",
    "    user = d[\"user\"]\n",
    "    weibos = d[\"weibo\"]\n",
    "    time_list = [pendulum.parse(w[\"publish_time\"]) for w in weibos]\n",
    "    recent = time_list[0]\n",
    "    span = (time_list[0]-time_list[len(time_list)-1]).days\n",
    "    if len(weibos)>=20 and recent.year>=2020 and span>=60:\n",
    "        users_n.append(user[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取原微博文件\n",
    "in_dir = \"data/weibo_data_456_json\"\n",
    "\n",
    "a = []\n",
    "b = []\n",
    "for i, in_name in enumerate(Path(in_dir).rglob(\"*.json\")):\n",
    "    if not in_name.name[:-5] in users_n:\n",
    "        continue\n",
    "    d = json.load(open(in_name,encoding='utf-8'))\n",
    "    uid = d[\"user\"][\"id\"]\n",
    "    all_weibo = \" \".join([w['content'] for w in d[\"weibo\"]])\n",
    "    all_desc = \" \".join(d[\"user\"][\"description\"])\n",
    "    all_weibo_desc = all_weibo +\" \"+ all_desc\n",
    "    a.append(uid)\n",
    "    b.append(all_weibo_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c={\"uid\" : a,\n",
    "   \"content\" : b} #将列表a，b转换成字典\n",
    "users_weibo = DataFrame(c) #将字典转换成为数据框\n",
    "content = users_weibo.iloc[:,1] #提取微博内容series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jieba分词，生成分好词的二维列表\n",
    "seg_list = []\n",
    "for i in range(len(content)):\n",
    "    seg = jieba.lcut(content[i])\n",
    "    seg_list.append(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每个句子所有词的特征向量求和再求平均\n",
    "vec_j = [0]*300\n",
    "word_vec1 = []\n",
    "for i in range(len(seg_list)):\n",
    "    for j in range(len(seg_list[i])):\n",
    "        if seg_list[i][j] in word_dict.keys():\n",
    "            vec_j = np.sum([vec_j, word_dict[seg_list[i][j]]], axis = 0).tolist()\n",
    "    vec_j = np.divide(vec_j, len(seg_list[i])).tolist()\n",
    "    word_vec1.append(vec_j)\n",
    "    vec_j = [0]*300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把word_vec1二维列表转换为数据框\n",
    "word_v = DataFrame(word_vec1)\n",
    "word_v.columns = [\"vec\"+str(i+1) for i in range(word_v.shape[1])]\n",
    "word_v.index = users_n\n",
    "word_v.index.name = \"uid\"\n",
    "#word_v.to_csv(\"features_202207/users320_vec_mean_features_202210.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据按均值标准差分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import pendulum\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"data/weibo_data_456_json\"\n",
    "\n",
    "users_n = []\n",
    "\n",
    "# 筛选出发微博超过20条的，且最近一条微博为2020年以后发布，且第一条与最后一条微博间隔天数超过60天的用户\n",
    "for i, in_name in enumerate(tqdm(Path(in_dir).rglob(\"*.json\"))):\n",
    "    d = json.load(open(in_name,encoding='utf-8'))\n",
    "    user = d[\"user\"]\n",
    "    weibos = d[\"weibo\"]\n",
    "    time_list = [pendulum.parse(w[\"publish_time\"]) for w in weibos]\n",
    "    recent = time_list[0]\n",
    "    span = (time_list[0]-time_list[len(time_list)-1]).days\n",
    "    if len(weibos)>=20 and recent.year>=2020 and span>=60:\n",
    "        users_n.append(user[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入问卷数据\n",
    "survey_data = pd.read_excel(\"data/456ID_data_0611.xlsx\")\n",
    "survey_data[\"userID_num\"].astype(\"str\")\n",
    "survey_data = survey_data.set_index(\"userID_num\")\n",
    "survey_data.index.name = \"uid\"\n",
    "\n",
    "users_nn = [int(i) for i in users_n]\n",
    "survey_data = survey_data[survey_data.index.isin(users_nn)]\n",
    "#survey_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data = survey_data[[c for c in survey_data.columns.to_list() if \"@\" not in c]]\n",
    "#survey_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [None]*320\n",
    "for index, c in survey_data.items():\n",
    "    if not index.startswith(\"DV\"):\n",
    "        continue\n",
    "    print(index)\n",
    "    c = [[_c] for _c in c]\n",
    "    \n",
    "    for i in range(0,len(c)):\n",
    "        if c[i] < mean(c)-std(c)/2:\n",
    "            y_pred[i] = 0\n",
    "        elif c[i] > mean(c)+std(c)/2:\n",
    "            y_pred[i] = 2\n",
    "        else:\n",
    "            y_pred[i] = 1\n",
    "\n",
    "    print(Counter(y_pred))\n",
    "    survey_data.loc[:, \"C3\" + index] = y_pred\n",
    "    \n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "#survey_data.to_csv(\"features_202207/users320_survey_emotion_clas_202210.csv\")\n",
    "#survey_data.to_csv(\"features_202207/users320_survey_emotion_clas_202210_gbk.csv\",encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_feas = pd.read_csv(\"features_202207/users320_b+t_features_202210.csv\")\n",
    "w_feas[\"uid\"].astype(\"str\")\n",
    "w_feas = w_feas.set_index(\"uid\")\n",
    "cols_feas1 = w_feas.columns\n",
    "#w_feas\n",
    "text_df = pd.read_csv(\"features_202207/users320_vec_mean_features_202210.csv\", index_col=\"uid\")\n",
    "cols_feas2 = text_df.columns\n",
    "#cols_feas2\n",
    "cols_feas = list(cols_feas1) + list(cols_feas2)\n",
    "survey_data = pd.read_csv(\"features_202207/users320_survey_emotion_clas_202210.csv\", index_col=\"uid\")\n",
    "all_data = survey_data.join(w_feas).join(text_df).copy()\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_data[cols_feas]\n",
    "print(\"y cols:\", [c for c in all_data.columns.to_list() if c.startswith(\"C3\")])\n",
    "y = all_data[[c for c in all_data.columns.to_list() if c.startswith(\"C3\")]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.model_selection import (GridSearchCV, cross_val_score,\n",
    "                                     cross_validate, train_test_split)\n",
    "from sklearn.metrics import auc, classification_report, f1_score, roc_curve\n",
    "\n",
    "from sklearn import linear_model,svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sjpc():\n",
    "\n",
    "    models = {\n",
    "        \"LR\": linear_model.LogisticRegression(C=1, solver=\"newton-cg\"),\n",
    "        \"K-Neighbors\": KNeighborsClassifier(n_neighbors= 3),\n",
    "        \"SVM\": svm.SVC(C=6, probability=True),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators = 4, max_depth = 5),\n",
    "        \"NN\": MLPClassifier((64, 32), max_iter=100, solver='lbfgs')\n",
    "    }\n",
    "\n",
    "    for model_name, clf in models.items():\n",
    "        rsts = {}\n",
    "        print(\"*\" * 100)\n",
    "        print(f'Model: {model_name}')\n",
    "\n",
    "        for col_name, y_i in y.iteritems():\n",
    "            if not  col_name.endswith(\"社交排斥\"):\n",
    "                continue\n",
    "            \n",
    "            y_i = np.array(y_i)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y_i, test_size=0.3, random_state=12)\n",
    "            \n",
    "            print(col_name)\n",
    "            clf.fit(X_train,y_train)\n",
    "            train_perf = clf.score(X_train, y_train)\n",
    "            cv_perf = cross_val_score(clf, X_train, y_train, cv=10).mean()\n",
    "\n",
    "            y_hat = clf.predict(X_test)\n",
    "            print('预测结果 =', Counter(y_hat))\n",
    "            print('实际结果 =', Counter(y_test))            \n",
    "            f1 = f1_score(y_test, y_hat, average='macro')\n",
    "\n",
    "            metrics = ('accuracy', 'roc_auc_ovr') \n",
    "            scores = cross_validate(clf, X, y_i, cv=10, scoring=metrics)\n",
    "            \n",
    "            rsts[col_name] = {\n",
    "                \"train dataset\": train_perf,\n",
    "                \"cross validation\": cv_perf,\n",
    "                \"test dataset (f1)\": f1,\n",
    "                \"accuracy\": scores['test_accuracy'].mean(),\n",
    "                \"roc_auc_ovr\": scores['test_roc_auc_ovr'].mean()                \n",
    "            }\n",
    "            \n",
    "        print(\"- * \" * 20)\n",
    "        \n",
    "        rsts = pd.DataFrame(rsts) * 100\n",
    "        rsts = rsts.T\n",
    "        display(rsts)\n",
    "        \n",
    "        #rsts.to_csv(f\"result2210/rsts1-sjpc-model={model_name}.csv\", float_format=\"%.4f\", encoding=\"gbk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sjpc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eyym():\n",
    "\n",
    "    models = {\n",
    "        \"LR\": linear_model.LogisticRegression(C=1, solver=\"newton-cg\"),\n",
    "        \"K-Neighbors\": KNeighborsClassifier(n_neighbors= 3),\n",
    "        \"SVM\": svm.SVC(C=6, probability=True),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators = 4, max_depth = 5),\n",
    "        \"NN\": MLPClassifier((64, 32), max_iter=100, solver='lbfgs')\n",
    "    }\n",
    "\n",
    "    for model_name, clf in models.items():\n",
    "        rsts = {}\n",
    "        print(\"*\" * 100)\n",
    "        print(f'Model: {model_name}')\n",
    "\n",
    "        for col_name, y_i in y.iteritems():\n",
    "            if not  col_name.endswith(\"恶意幽默\"):\n",
    "                continue\n",
    "            \n",
    "            y_i = np.array(y_i)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y_i, test_size=0.3, random_state=12)\n",
    "            \n",
    "            print(col_name)\n",
    "            clf.fit(X_train,y_train)\n",
    "            train_perf = clf.score(X_train, y_train)\n",
    "            cv_perf = cross_val_score(clf, X_train, y_train, cv=10).mean()\n",
    "\n",
    "            y_hat = clf.predict(X_test)\n",
    "            print('预测结果 =', Counter(y_hat))\n",
    "            print('实际结果 =', Counter(y_test))            \n",
    "            f1 = f1_score(y_test, y_hat, average='macro')\n",
    "\n",
    "            metrics = ('accuracy', 'roc_auc_ovr') \n",
    "            scores = cross_validate(clf, X, y_i, cv=10, scoring=metrics)\n",
    "            \n",
    "            rsts[col_name] = {\n",
    "                \"train dataset\": train_perf,\n",
    "                \"cross validation\": cv_perf,\n",
    "                \"test dataset (f1)\": f1,\n",
    "                \"accuracy\": scores['test_accuracy'].mean(),\n",
    "                \"roc_auc_ovr\": scores['test_roc_auc_ovr'].mean()                \n",
    "            }\n",
    "            \n",
    "        print(\"- * \" * 20)\n",
    "        \n",
    "        rsts = pd.DataFrame(rsts) * 100\n",
    "        rsts = rsts.T\n",
    "        display(rsts)\n",
    "        \n",
    "        #rsts.to_csv(f\"result2210/rsts1-eyym-model={model_name}.csv\", float_format=\"%.4f\", encoding=\"gbk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eyym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_njyd():\n",
    "\n",
    "    models = {\n",
    "        \"LR\": linear_model.LogisticRegression(C=1, solver=\"newton-cg\"),\n",
    "        \"K-Neighbors\": KNeighborsClassifier(n_neighbors= 3),\n",
    "        \"SVM\": svm.SVC(C=6, probability=True),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators = 4, max_depth = 5),\n",
    "        \"NN\": MLPClassifier((64, 32), max_iter=100, solver='lbfgs')\n",
    "    }\n",
    "\n",
    "    for model_name, clf in models.items():\n",
    "        rsts = {}\n",
    "        print(\"*\" * 100)\n",
    "        print(f'Model: {model_name}')\n",
    "\n",
    "        for col_name, y_i in y.iteritems():\n",
    "            if not  col_name.endswith(\"内疚诱导\"):\n",
    "                continue\n",
    "            \n",
    "            y_i = np.array(y_i)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y_i, test_size=0.3, random_state=12)\n",
    "            \n",
    "            print(col_name)\n",
    "            clf.fit(X_train,y_train)\n",
    "            train_perf = clf.score(X_train, y_train)\n",
    "            cv_perf = cross_val_score(clf, X_train, y_train, cv=10).mean()\n",
    "\n",
    "            y_hat = clf.predict(X_test)\n",
    "            print('预测结果 =', Counter(y_hat))\n",
    "            print('实际结果 =', Counter(y_test))            \n",
    "            f1 = f1_score(y_test, y_hat, average='macro')\n",
    "\n",
    "            metrics = ('accuracy', 'roc_auc_ovr') \n",
    "            scores = cross_validate(clf, X, y_i, cv=10, scoring=metrics)\n",
    "            \n",
    "            rsts[col_name] = {\n",
    "                \"train dataset\": train_perf,\n",
    "                \"cross validation\": cv_perf,\n",
    "                \"test dataset (f1)\": f1,\n",
    "                \"accuracy\": scores['test_accuracy'].mean(),\n",
    "                \"roc_auc_ovr\": scores['test_roc_auc_ovr'].mean()                \n",
    "            }\n",
    "            \n",
    "        print(\"- * \" * 20)\n",
    "        \n",
    "        rsts = pd.DataFrame(rsts) * 100\n",
    "        rsts = rsts.T\n",
    "        display(rsts)\n",
    "        \n",
    "        #rsts.to_csv(f\"result2210/rsts1-njyd-model={model_name}.csv\", float_format=\"%.4f\", encoding=\"gbk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_njyd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Jun  1 2022, 06:34:44) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab8c153ce49f82bb1ed29f2c31039786033a9fbd0ab1fbce815ac3f49b9f9c7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
